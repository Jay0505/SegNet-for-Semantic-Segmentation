{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "def get_label_color_values(label_folder_path):\n",
    "    label_images_names = os.listdir(label_folder_path)\n",
    "    rgb_values = get_pixel_RGB_values(label_folder_path + '/' + label_images_names[0])\n",
    "    \n",
    "    return rgb_values\n",
    "\n",
    "\n",
    "\n",
    "def get_pixel_RGB_values(path):\n",
    "    im = Image.open(path)\n",
    "    rgb_values = []\n",
    "    pix = im.load()\n",
    "    \n",
    "    return list(set(list(im.getdata())))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_image_to_one_hot(image, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        # colour_map = np.full((label.shape[0], label.shape[1], label.shape[2]), colour, dtype=int)\n",
    "        equality = np.equal(image, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_input_batches(images_path, labels_path):\n",
    "    \n",
    "    def generate_batches(batch_size, label_values):\n",
    "        images_names = os.listdir(images_path)\n",
    "        total_no_of_images = len(images_names)\n",
    "\n",
    "        for index in range(0, total_no_of_images, batch_size):\n",
    "            images_arr = []\n",
    "            labels_arr = []\n",
    "            for image_name in images_names[index : index + batch_size]:\n",
    "                image = cv2.resize(cv2.imread(images_path + \"/\" + image_name), (620, 180))\n",
    "                label = cv2.resize(cv2.imread(labels_path + \"/\" + image_name), (620, 180))\n",
    "\n",
    "                image = image / 255.0\n",
    "                label_ = label_image_to_one_hot(image, label_values)\n",
    "    \n",
    "                images_arr.append(image)\n",
    "                labels_arr.append(label_)\n",
    "\n",
    "            yield np.asarray(images_arr), np.asarray(labels_arr)\n",
    "    \n",
    "    return generate_batches\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = get_label_color_values('/Users/vijay/Downloads/Datasets/data_semantics/training/semantic_rgb')\n",
    "get_batches = get_input_batches('/Users/vijay/Downloads/Datasets/data_semantics/training/image_2', \n",
    "                                '/Users/vijay/Downloads/Datasets/data_semantics/training/semantic_rgb')\n",
    "\n",
    "\n",
    "num_of_classes = len(label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################\n",
    "\n",
    "def get_scale_and_gamma_variables(shape):\n",
    "\n",
    "    scale = tf.Variable(tf.ones(shape))\n",
    "    offset = tf.Variable(tf.zeros(shape))\n",
    "    \n",
    "    return scale, offset\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def batch_normalize(data, shape):\n",
    "\n",
    "    scale, offset = get_scale_and_gamma_variables(shape)\n",
    "    mean, var     = tf.nn.moments(data, axes = [0, 1, 2])\n",
    "    data_norm     = tf.nn.batch_normalization(data, mean, var, scale, offset, 0.05, name = \"BN\")\n",
    "    \n",
    "    return data_norm\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def get_weights_and_bias(wts_shape, wts_name, bias_name):\n",
    "#     print('entered')\n",
    "#     wts_init = tf.truncated_normal(wts_shape)\n",
    "    wts_init = tf.truncated_normal_initializer()\n",
    "    bias_init = tf.constant_initializer(0.1)\n",
    "    \n",
    "    wts = tf.get_variable(name = wts_name, shape = wts_shape, initializer = wts_init)\n",
    "    bias = tf.get_variable(name = bias_name, shape = [wts_shape[-1]], initializer = bias_init)\n",
    "    \n",
    "    return wts, bias\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def conv_layer(data, weights, bias, name):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        conv_res      = tf.nn.conv2d(data, filter = weights, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "        conv_res_bias = tf.nn.bias_add(conv_res, bias)\n",
    "        conv_bn       = batch_normalize(conv_res_bias, conv_res_bias.get_shape().as_list()[-1])\n",
    "        conv_bn_relu = tf.nn.relu(conv_bn)\n",
    "        \n",
    "        return conv_bn_relu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "\n",
    "def max_pooling_with_arg_max(input_data,  pool_name, filter_size = 2, stride = 2):\n",
    "    filter_shape = [1, filter_size, filter_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    \n",
    "    with tf.variable_scope(pool_name, reuse=tf.AUTO_REUSE):\n",
    "        _, max_indices = tf.nn.max_pool_with_argmax(input_data, ksize = filter_shape, strides = strides, padding = 'SAME')\n",
    "        pool_res       = tf.nn.max_pool(input_data, ksize = filter_shape, strides = strides, padding = 'SAME')\n",
    "        max_indices    = tf.stop_gradient(max_indices)\n",
    "    \n",
    "    return pool_res, max_indices\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "############################################\n",
    "\n",
    "def encoder_block(data, filter_size, no_of_filters, wts_name, bias_name, block_name, no_of_blocks = 0):\n",
    "    conv_data      = data\n",
    "    \n",
    "    for block_no in range(0, no_of_blocks):\n",
    "        \n",
    "        data_shape = conv_data.get_shape().as_list()\n",
    "        wts_shape  = [filter_size, filter_size, data_shape[-1], no_of_filters]\n",
    "        wt_name    = wts_name + '_' + str(block_no)\n",
    "        bs_name    = bias_name + '_' + str(block_no)\n",
    "        wts, bias  = get_weights_and_bias(wts_shape, wt_name, bs_name)\n",
    "        conv_data  = conv_layer(conv_data, wts, bias, (block_name + '_' + str(block_no)))\n",
    "    \n",
    "    pool_res, max_indices = max_pooling_with_arg_max(conv_data, (block_name + 'pool'))\n",
    "    return pool_res, max_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "    \n",
    "def encoder(inputs, encoder_name):\n",
    "    with tf.variable_scope(encoder_name, reuse=tf.AUTO_REUSE):\n",
    "        pool_1, max_indices_1 = encoder_block(inputs, 3, 64, 'w_1', 'b_1', 'en_blk_1', 2)\n",
    "        pool_2, max_indices_2 = encoder_block(pool_1, 3, 128, 'w_2', 'b_2', 'en_blk_2', 2)\n",
    "        pool_3, max_indices_3 = encoder_block(pool_2, 3, 256, 'w_3', 'b_3', 'en_blk_3', 3)\n",
    "        pool_4, max_indices_4 = encoder_block(pool_3, 3, 512, 'w_4', 'b_4', 'en_blk_4', 3)\n",
    "        pool_5, max_indices_5 = encoder_block(pool_4, 3, 512, 'w_5', 'b_5', 'en_blk_5', 3)\n",
    "        \n",
    "        return pool_5, [max_indices_5, max_indices_4, max_indices_3, max_indices_2, max_indices_1]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECODER BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def form_weights_and_get_weights_and_biases(data_shape, no_of_filters, filter_size, wts_name, bias_name):\n",
    "    wts_shape = [filter_size, filter_size, data_shape[-1], no_of_filters]\n",
    "    wts, bias = get_weights_and_bias(wts_shape, wts_name, bias_name)\n",
    "    return wts, bias\n",
    "\n",
    "\n",
    "\n",
    "def flat_the_shape(shape_of_conv_out):\n",
    "\n",
    "    neurons = 1\n",
    "    for num in shape_of_conv_out:\n",
    "        if num is not None:\n",
    "            neurons = neurons * num\n",
    "\n",
    "    return neurons\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "# https://github.com/Pepslee/tensorflow-contrib/blob/master/unpooling.py\n",
    "def max_unpooling(input_data, unpool_indices, name):\n",
    "    \n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "#         print('unpool indices ' + str(unpool_indices.get_shape().as_list()))\n",
    "#         print('input_data ' + str(input_data.get_shape().as_list()))\n",
    "        stride = 2\n",
    "        input_shape = tf.shape(input_data)\n",
    "        print('input_shape is ' + str(input_shape))\n",
    "        print('--------------------------------')\n",
    "\n",
    "        output_shape = [input_shape[0], input_shape[1] * stride, \n",
    "                        input_shape[2] * stride, input_shape[3]]\n",
    "    #     output_shape = [batch_size, input_shape[1] * stride, input_shape[2] * stride, input_shape[3]]\n",
    "        print('output shpae ' + str(output_shape))\n",
    "\n",
    "    #     output_shape_flatten = [output_shape[0], flat_the_shape(output_shape[1:])]\n",
    "        output_shape_flatten = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n",
    "        input_shape_flatten = tf.reduce_prod(input_shape)\n",
    "#         print('input shape ' + str(input_shape))\n",
    "#         print('flat inpyt '  + str(input_shape_flatten))\n",
    "        input_rs = tf.reshape(input_data, [input_shape_flatten])\n",
    "        batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=unpool_indices.dtype), shape=[input_shape[0], 1, 1, 1])\n",
    "#         batch_range = tf.reshape(tf.range(batch_size, dtype=unpool_indices.dtype), shape=[batch_size, 1, 1, 1])\n",
    "#         print('batch range is ' + str(batch_range.get_shape().as_list()))\n",
    "        b = tf.ones_like(unpool_indices) * batch_range\n",
    "#         b = tf.ones_like(unpool_indices)\n",
    "#         print('b ' + str(b.get_shape().as_list()))\n",
    "        b1 = tf.reshape(b, [input_shape_flatten, 1])\n",
    "        ind_ = tf.reshape(unpool_indices, [input_shape_flatten, 1])\n",
    "        ind_ = tf.concat([b1, ind_], 1)\n",
    "#         print('shape of ind_ ' + str(ind_.get_shape().as_list()) + ' ' + str(input_rs.get_shape().as_list()))\n",
    "#         ret = tf.scatter_nd(ind_, input_rs, shape = output_shape_flatten)\n",
    "        ret = tf.scatter_nd(ind_, input_rs, shape = tf.cast(output_shape_flatten, tf.int64))\n",
    "        print('ret shape ' + str(ret.get_shape().as_list()))\n",
    "        print('output shape ' + str(output_shape))\n",
    "        ret = tf.reshape(ret, output_shape)\n",
    "        \n",
    "        set_input_shape = input_data.get_shape().as_list()\n",
    "        set_output_shape = [set_input_shape[0], set_input_shape[1] * 2, set_input_shape[2] * 2, set_input_shape[3]]\n",
    "        ret.set_shape(set_output_shape)\n",
    "#         print('ret ' + str(ret.get_shape().as_list()))\n",
    "#         ret = tf.reshape(ret, (set_output_shape))\n",
    "        return ret\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "def decoder_block(data, unpool_indices, shrink_no_of_filters, no_of_filters, block_name, wts_name, bias_name, \n",
    "                                                                   no_of_blocks = 0):\n",
    "#     unpool_conv_data = data\n",
    "    filter_size = 3\n",
    "    with tf.variable_scope(block_name, reuse=tf.AUTO_REUSE):\n",
    "        unpool_conv_data = max_unpooling(data, unpool_indices, 'unpool')\n",
    "        \n",
    "        for block_no in range(0, no_of_blocks):\n",
    "            \n",
    "            data_shape = unpool_conv_data.get_shape().as_list()\n",
    "#             print('data _sahpe ' + str(data_shape))\n",
    "            wt_name    = wts_name + '_' + str(block_no)\n",
    "            bs_name    = bias_name + '_' + str(block_no)\n",
    "            \n",
    "            if block_no  == no_of_blocks - 1 and shrink_no_of_filters:\n",
    "                wts_shape  = [filter_size, filter_size, data_shape[-1], (no_of_filters / 2)]  \n",
    "#                 print('wts_shape ' + str(wts_shape))\n",
    "                wts, bias = get_weights_and_bias(wts_shape, wt_name, bs_name)\n",
    "                unpool_conv_data = conv_layer(unpool_conv_data, wts, bias, ('deconv_' + str(block_no)))\n",
    "                return unpool_conv_data\n",
    "            else:\n",
    "                wts_shape  = [filter_size, filter_size, data_shape[-1], no_of_filters]\n",
    "#                 print('wts_shape ' + str(wts_shape))\n",
    "                wts, bias = get_weights_and_bias(wts_shape, wt_name, bs_name)\n",
    "                unpool_conv_data = conv_layer(unpool_conv_data, wts, bias, ('deconv_' + str(block_no)))\n",
    "                return unpool_conv_data\n",
    "            \n",
    "#             wts, bias = get_weights_and_bias(wts_shape, wt_name, bs_name)\n",
    "#             unpool_conv_data = conv_layer(unpool_conv_data, wts, bias, ('deconv_' + str(block_no)))\n",
    "        \n",
    "#         return unpool_conv_data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "############################################\n",
    "\n",
    "def deconv_last_step(inputs, no_of_filters, filter_size, block_name, wts_name, bias_name):\n",
    "    data_shape = inputs.get_shape().as_list()\n",
    "    with tf.variable_scope(block_name, reuse=tf.AUTO_REUSE):\n",
    "        wts_shape      = [filter_size, filter_size, data_shape[-1], no_of_filters]\n",
    "        wts, bias      = get_weights_and_bias(wts_shape, wts_name, bias_name)\n",
    "        last_conv      = tf.nn.conv2d(inputs, filter = wts, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "        last_conv_bias = tf.nn.bias_add(last_conv, bias)\n",
    "        return tf.identity(last_conv_bias)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "############################################\n",
    "\n",
    "def decoder(inputs, max_indices, decoder_name):\n",
    "    upsample_res_1 = decoder_block(inputs, max_indices[0], False, 512, 'de_blk_5', 'de_w_5', 'de_b_5', 3)\n",
    "    upsample_res_2 = decoder_block(upsample_res_1, max_indices[1], True, 512, 'de_blk_4', 'de_w_4', 'de_b_4', 3)\n",
    "    upsample_res_3 = decoder_block(upsample_res_2, max_indices[2], True, 256, 'de_blk_3', 'de_w_3', 'de_b_3', 3)\n",
    "    upsample_res_4 = decoder_block(upsample_res_3, max_indices[3], True, 128, 'de_blk_2', 'de_w_2', 'de_b_2', 2)\n",
    "    upsample_res_5 = decoder_block(upsample_res_4, max_indices[4], True, 64, 'de_blk_1', 'de_w_1', 'de_b_1', 2)\n",
    "    logits         = deconv_last_step(upsample_res_5, num_of_classes, 1, 'last_conv', 'l_w', 'l_b')\n",
    "    return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X       = tf.placeholder(tf.float32, shape = [None, 180, 620, 3], name = 'x_rs')\n",
    "Y       = tf.placeholder(tf.float32, shape = [None, 180, 620, num_of_classes])\n",
    "dropout = tf.placeholder(tf.float32, name = 'dropout')\n",
    "\n",
    "\n",
    "def encode_and_then_decode(inputs):\n",
    "    pool_5, max_indices_list = encoder(inputs, 'encoder')\n",
    "#     print('pool _ 5 ' + str(pool_5.get_shape().as_list()))\n",
    "    logits      = decoder(pool_5, max_indices_list, 'decoder')\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "    \n",
    "def logistic_loss(logits, labels, n_classes):\n",
    "    \n",
    "    with tf.variable_scope('logistic_loss'):\n",
    "        \n",
    "        reshaped_logits = tf.reshape(logits, (-1, n_classes))\n",
    "        reshaped_labels = tf.reshape(labels, (-1, n_classes))\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=reshaped_logits,\n",
    "                                                          labels=reshaped_labels)\n",
    "        loss = tf.reduce_mean(entropy, name='logistic_loss')\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_optimizer_and_get_accuracy():\n",
    "    with tf.variable_scope('optimizer', reuse = tf.AUTO_REUSE):\n",
    "        logits    = encode_and_then_decode(X)\n",
    "        cost      = logistic_loss(logits, Y, num_of_classes)\n",
    "    #     cost      = tf.reduce_mean(tf.square(Y - logits))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "        return logits, cost, optimizer\n",
    "\n",
    "\n",
    "def run_SegNet_model():\n",
    "    with tf.Session() as session:\n",
    "        logits, loss, optimizer = train_optimizer_and_get_accuracy()\n",
    "        \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        rows = np.shape(X)[0]\n",
    "        for epoch in range(0, 1):\n",
    "            images_arr, labels_arr = next(get_batches(batch_size, label_values))\n",
    "            print(np.shape(images_arr), np.shape(labels_arr))\n",
    "            _, loss_ = session.run([optimizer, loss], feed_dict = {X : images_arr ,\n",
    "                                                                   Y : labels_arr, \n",
    "                                                                   dropout : 0.5})\n",
    "                \n",
    "            print(epoch, loss_)\n",
    "        save_path = saver.save(session, \"./SS_SEGNET_model\")\n",
    "        \n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334800"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "180 * 620 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segnet_logits = run_SegNet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    var = tf.Variable(tf.truncated_normal([6, 6, 6, 6], stddev = 0.02))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_range = tf.reshape(tf.range(6, dtype=tf.int32),\n",
    "                                 shape=[6, 1, 1, 1])\n",
    "    print(batch_range.get_shape().as_list())\n",
    "    one = tf.ones_like(var)\n",
    "    print(one.get_shape().as_list())\n",
    "    b = one * batch_range\n",
    "    print(b.get_shape().as_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
